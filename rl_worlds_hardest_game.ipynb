{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2 #opencv\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from random import randint\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "#keras imports\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path variables\n",
    "#game_url = \"https://www.coolmathgames.com/0-worlds-hardest-game/play\"\n",
    "# use if offline\n",
    "#game_url = \"C:/Users/mrchr/OneDrive/Documents/worlds-hardest-game-ai/game/index.html\" \n",
    "\n",
    "# run first --> cd C:/Users/mrchr/OneDrive/Documents/worlds-hardest-game-ai/ \n",
    "# run --> python -m http.server 8080\n",
    "\n",
    "game_url = \"http://localhost:8080/game/\" \n",
    "\n",
    "chrome_driver_path = \"res/chromedriver\"\n",
    "loss_file_path = \"./objects/loss_df.csv\"\n",
    "actions_file_path = \"./objects/actions_df.csv\"\n",
    "q_value_file_path = \"./objects/q_values.csv\"\n",
    "\n",
    "#scripts\n",
    "\n",
    "#get image from canvas\n",
    "getbase64Script = \"canvasRunner = document.getElementById('twhgCanvas'); \\\n",
    "return canvasRunner.toDataURL().substring(22);\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "* Game class: Selenium interfacing between the python and browser\n",
    "* __init__():  Launch the broswer window using the attributes in chrome_options\n",
    "* get_crashed() : return true if the agent as crashed on an obstacles. Gets javascript variable from game decribing the state\n",
    "* get_playing(): true if game in progress, false is crashed or paused\n",
    "* press_up(): sends a single to press up get to the browser\n",
    "* pause(): pause the game\n",
    "* resume(): resume a paused game if not crashed\n",
    "* end(): close the browser and end the game\n",
    "'''\n",
    "class Game:\n",
    "    def __init__(self,custom_config=True):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        chrome_options.add_argument(\"--mute-audio\")\n",
    "        self._driver = webdriver.Chrome(executable_path = chrome_driver_path,chrome_options=chrome_options)\n",
    "        self._driver.set_window_position(x=-10,y=0)\n",
    "        self._driver.set_window_size(810, 600)\n",
    "        self._driver.get(game_url)\n",
    "    def get_crashed(self):\n",
    "        return self._driver.execute_script(\"return player.dying\")\n",
    "    def get_playing(self):\n",
    "        return self._driver.execute_script(\"return state == 'game' && !player.dying\")\n",
    "    def get_completed(self):\n",
    "        if self._driver.execute_script(\"return finishLevelTimer > 0\"):\n",
    "            self._driver.execute_script(\"finishLevelTimer = 1\")\n",
    "            return True\n",
    "        return False\n",
    "    def press_up(self, stride):\n",
    "        start_y = self._driver.execute_script(\"return player.y\")\n",
    "        prev_y = -1\n",
    "        curr_y = start_y\n",
    "        while curr_y >= start_y - stride:\n",
    "            if curr_y == prev_y:\n",
    "                self._driver.execute_script(\"keydown.up = false\")\n",
    "                self._driver.execute_script(\"keyUp = false\")\n",
    "                return True\n",
    "            prev_y = curr_y\n",
    "            self._driver.execute_script(\"keyUp = true\")\n",
    "            self._driver.execute_script(\"keydown.up = true\")\n",
    "            time.sleep(0.025)\n",
    "            curr_y = self._driver.execute_script(\"return player.y\")\n",
    "        self._driver.execute_script(\"keydown.up = false\")\n",
    "        self._driver.execute_script(\"keyUp = false\")\n",
    "        return False\n",
    "    def press_down(self, stride):\n",
    "        start_y = self._driver.execute_script(\"return player.y\")\n",
    "        prev_y = -1\n",
    "        curr_y = start_y\n",
    "        while curr_y < start_y + stride:\n",
    "            if curr_y == prev_y:\n",
    "                self._driver.execute_script(\"keydown.down = false\")\n",
    "                self._driver.execute_script(\"keyDown = false\")\n",
    "                return True\n",
    "            prev_y = curr_y\n",
    "            self._driver.execute_script(\"keyDown = true\")\n",
    "            self._driver.execute_script(\"keydown.down = true\")\n",
    "            time.sleep(0.025)\n",
    "            curr_y = self._driver.execute_script(\"return player.y\")\n",
    "        self._driver.execute_script(\"keydown.down = false\")\n",
    "        self._driver.execute_script(\"keyDown = false\")\n",
    "        return False\n",
    "    def press_left(self, stride):\n",
    "        start_x = self._driver.execute_script(\"return player.x\")\n",
    "        prev_x = -1\n",
    "        curr_x = start_x\n",
    "        while curr_x >= start_x - stride:\n",
    "            if curr_x == prev_x:\n",
    "                self._driver.execute_script(\"keydown.left = false\")\n",
    "                self._driver.execute_script(\"keyLeft = false\")\n",
    "                return True\n",
    "            prev_x = curr_x\n",
    "            self._driver.execute_script(\"keyLeft = true\")\n",
    "            self._driver.execute_script(\"keydown.left = true\")\n",
    "            time.sleep(0.025)\n",
    "            curr_x = self._driver.execute_script(\"return player.x\")\n",
    "        self._driver.execute_script(\"keydown.left = false\")\n",
    "        self._driver.execute_script(\"keyLeft = false\")\n",
    "        return False\n",
    "    def press_right(self, stride): # 120ms to 50ms to move\n",
    "        start_x = self._driver.execute_script(\"return player.x\")\n",
    "        prev_x = -1\n",
    "        curr_x = start_x\n",
    "        while curr_x < start_x + stride:\n",
    "            if curr_x == prev_x:\n",
    "                self._driver.execute_script(\"keydown.right = false\")\n",
    "                self._driver.execute_script(\"keyRight = false\")\n",
    "                return True\n",
    "            prev_x = curr_x\n",
    "            self._driver.execute_script(\"keyRight = true\")\n",
    "            self._driver.execute_script(\"keydown.right = true\")\n",
    "            time.sleep(0.025)\n",
    "            curr_x = self._driver.execute_script(\"return player.x\")\n",
    "        self._driver.execute_script(\"keydown.right = false\")\n",
    "        self._driver.execute_script(\"keyRight = false\")\n",
    "        return False\n",
    "    def pause(self):\n",
    "        return self._driver.execute_script(\"paused = true\")\n",
    "    def resume(self):\n",
    "        return self._driver.execute_script(\"paused = false\")\n",
    "    def end(self):\n",
    "        self._driver.close()\n",
    "    def enter_level(self):\n",
    "        time.sleep(0.50) # wait for things to load\n",
    "        while self._driver.execute_script(\"return state !== 'game'\"):\n",
    "            self._driver.find_element_by_id(\"twhgCanvas\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrchr\\Miniconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: use options instead of chrome_options\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "game = Game()\n",
    "agent = Agent(game)\n",
    "game_state = Game_sate(agent,game)\n",
    "game.enter_level()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.up()\n",
    "agent.down()\n",
    "agent.left()\n",
    "agent.right()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,game): #takes game as input for taking actions\n",
    "        self._game = game;\n",
    "        self.stride = 32\n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n",
    "    def won(self):\n",
    "        return self._game.get_completed()\n",
    "    def up(self):\n",
    "        return self._game.press_up(self.stride)\n",
    "    def down(self):\n",
    "        return self._game.press_down(self.stride)\n",
    "    def left(self):\n",
    "        return self._game.press_left(self.stride)\n",
    "    def right(self):\n",
    "        return self._game.press_right(self.stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_sate:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = show_img() #display the processed image on screen using openCV, implemented using python coroutine \n",
    "        self._display.__next__() # initiliaze the display coroutine\n",
    "    def get_block_map(self):\n",
    "        screen = grab_screen(self._game._driver)\n",
    "        hsv = cv2.cvtColor(screen, cv2.COLOR_BGR2HSV)\n",
    "        block_map = np.zeros([13, 20])\n",
    "\n",
    "        # purplish outside\n",
    "        mask1 = cv2.inRange(hsv, (95, 50, 50), (135, 255, 255))\n",
    "        grey_img = cv2.cvtColor(cv2.bitwise_and(screen, screen, mask=mask1), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        for r in range(grey_img.shape[0]//4):\n",
    "            for c in range(grey_img.shape[1]//4):\n",
    "                if np.any(grey_img[r*4:r*4+4, c*4:c*4+4] >= 128):\n",
    "                    block_map[r, c] = -1\n",
    "\n",
    "        # red\n",
    "        mask2 = cv2.inRange(hsv, (0, 50, 50), (18, 255, 255))\n",
    "        mask2 = cv2.bitwise_or(mask2, cv2.inRange(hsv, (165, 50, 50), (180, 255, 255)))\n",
    "        grey_img = cv2.cvtColor(cv2.bitwise_and(screen, screen, mask=mask2), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        for r in range(grey_img.shape[0]//4):\n",
    "            for c in range(grey_img.shape[1]//4):\n",
    "                if np.any(grey_img[r*4:r*4+4, c*4:c*4+4] >= 25) and block_map[r, c] == 0:\n",
    "                    block_map[r, c] = 1\n",
    "\n",
    "        goal_location = self._game._driver.execute_script(\"return checkpoints[level][checkpoints[level].length - 1]\")\n",
    "\n",
    "        for c in range(goal_location[0], goal_location[0] + goal_location[2]):\n",
    "            for r in range(goal_location[1] - 1, goal_location[1] - 1 + goal_location[3]):\n",
    "                block_map[r, c] = 100\n",
    "\n",
    "        return block_map\n",
    "    def get_state(self,actions):\n",
    "        actions_df.loc[len(actions_df)] = actions[1] # storing actions in a dataframe\n",
    "        score = 1\n",
    "        action_reward = -0.2\n",
    "        is_over = False #game over\n",
    "        if actions[1] == 1:\n",
    "            if self._agent.up():\n",
    "                action_reward = -1\n",
    "            else:\n",
    "                action_reward = -0.1\n",
    "        elif actions[2] == 1:\n",
    "            if self._agent.down():\n",
    "                action_reward = -1\n",
    "            else:\n",
    "                action_reward = -0.1\n",
    "        elif actions[3] == 1:\n",
    "            if self._agent.left():\n",
    "                action_reward = -1\n",
    "            else:\n",
    "                action_reward = -0.1\n",
    "        elif actions[4] == 1:\n",
    "            if self._agent.right():\n",
    "                action_reward = -1\n",
    "            else:\n",
    "                action_reward = -0.1\n",
    "        \n",
    "        image = self.get_block_map()\n",
    "        self._display.send(image) #display the image on screen\n",
    "        \n",
    "        if self._agent.is_crashed():\n",
    "            action_reward = -100\n",
    "            is_over = True\n",
    "        elif self._agent.won():\n",
    "            action_reward = 100\n",
    "            print(\"WE JUST WON\")\n",
    "            is_over = True\n",
    "        \n",
    "        return image, action_reward , is_over #return the Experience tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('objects/'+ name + '.pkl', 'wb') as f: #dump files into objects folder\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_obj(name ):\n",
    "    with open('objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def grab_screen(_driver):\n",
    "    image_b64 = _driver.execute_script(getbase64Script)\n",
    "    screen = np.array(Image.open(BytesIO(base64.b64decode(image_b64))))\n",
    "    image = process_img(screen)#processing image as required\n",
    "    return image\n",
    "\n",
    "def process_img(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #BGR to RGB\n",
    "    image = image[50:550, :] #Crop Region of Interest(ROI)\n",
    "    image = cv2.resize(image, (80, 52))\n",
    "    return  image\n",
    "\n",
    "def show_img(graphs = False):\n",
    "    \"\"\"\n",
    "    Show images in new window\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(window_title, 200, 130)\n",
    "        cv2.imshow(window_title, screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize log structures from file if exists else create new\n",
    "loss_df = pd.read_csv(loss_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns =['loss'])\n",
    "actions_df = pd.read_csv(actions_file_path) if os.path.isfile(actions_file_path) else pd.DataFrame(columns = ['actions'])\n",
    "q_values_df =pd.read_csv(actions_file_path) if os.path.isfile(q_value_file_path) else pd.DataFrame(columns = ['qvalues'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game parameters\n",
    "ACTIONS = 5 # possible actions: up, down, left, right, do nothing\n",
    "GAMMA = 0.99 # decay rate of past observations original 0.99\n",
    "OBSERVATION = 100. # timesteps to observe before training\n",
    "EXPLORE = 100000  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 16 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "img_rows , img_cols = 20, 13\n",
    "img_channels = 4 #We stack 4 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training variables saved as checkpoints to filesystem to resume training from the same step\n",
    "def init_cache():\n",
    "    \"\"\"initial variable caching, done only once\"\"\"\n",
    "    save_obj(INITIAL_EPSILON,\"epsilon\")\n",
    "    t = 0\n",
    "    save_obj(t,\"time\")\n",
    "    D = deque()\n",
    "    save_obj(D,\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Call only once to init file structure\n",
    "'''\n",
    "init_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildmodel():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), padding='same',strides=(4, 4),input_shape=(img_cols,img_rows,img_channels)))  #100*63*4\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Conv2D(64, (4, 4),strides=(2, 2),  padding='same'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Conv2D(64, (3, 3),strides=(1, 1),  padding='same'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    #model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    \n",
    "    #create model file if not present\n",
    "    if not os.path.isfile(loss_file_path):\n",
    "        model.save_weights('model.h5')\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "main training module\n",
    "Parameters:\n",
    "* model => Keras Model to be trained\n",
    "* game_state => Game State module with access to game environment and dino\n",
    "* observe => flag to indicate wherther the model is to be trained(weight updates), else just play\n",
    "'''\n",
    "def trainNetwork(model,game_state,observe=False):\n",
    "    last_time = time.time()\n",
    "    # store the previous observations in replay memory\n",
    "    D = load_obj(\"D\") #load from file system\n",
    "    # get the first state by doing nothing\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] =1 #0 => do nothing,\n",
    "                     #1=> jump\n",
    "    \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing) # get next step after performing the action\n",
    "    \n",
    "\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) # stack 4 images to create placeholder input\n",
    "    \n",
    "\n",
    "    \n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*20*40*4\n",
    "    \n",
    "    initial_state = s_t \n",
    "\n",
    "    if observe :\n",
    "        OBSERVE = 999999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = load_obj(\"epsilon\") \n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "    t = load_obj(\"time\") # resume from the previous time step stored in file system\n",
    "    while (True): #endless running\n",
    "        \n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0 #reward at 4\n",
    "        a_t = np.zeros([ACTIONS]) # action at t\n",
    "        \n",
    "        #choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0: #parameter to skip frames for actions\n",
    "            if  random.random() <= epsilon: #randomly explore an action\n",
    "                print(\"----------Random Action----------\")\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[action_index] = 1\n",
    "            else: # predict the output\n",
    "                q = model.predict(s_t)       #input a stack of 4 images, get the prediction\n",
    "                max_Q = np.argmax(q)         # chosing index with maximum q value\n",
    "                action_index = max_Q \n",
    "                a_t[action_index] = 1        # o=> do nothing, 1=> jump\n",
    "                \n",
    "        #We reduced the epsilon (exploration parameter) gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE \n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        print('fps: {0}'.format(1 / (time.time()-last_time))) # helpful for measuring frame rate\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x20x40x1\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) # append the new image to input stack and remove the first one\n",
    "        \n",
    "        \n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE: \n",
    "            \n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]    # 4D stack of images\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]   #reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]   #next state\n",
    "                terminal = minibatch[i][4]   #wheather the agent died or survided due the action\n",
    "                \n",
    "\n",
    "                inputs[i:i + 1] = state_t    \n",
    "\n",
    "                targets[i] = model.predict(state_t)  # predicted q values\n",
    "                Q_sa = model.predict(state_t1)      #predict q values for next step\n",
    "                \n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t # if terminated, only equals reward\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "            q_values_df.loc[len(q_values_df)] = np.max(Q_sa)\n",
    "        s_t = initial_state if terminal else s_t1 #reset game to initial frame if terminate\n",
    "        t = t + 1\n",
    "        \n",
    "        # save progress every 1000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            game_state._game.pause() #pause game while saving to filesystem\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "            save_obj(D,\"D\") #saving episodes\n",
    "            save_obj(t,\"time\") #caching time steps\n",
    "            save_obj(epsilon,\"epsilon\") #cache epsilon to avoid repeated randomness in actions\n",
    "            loss_df.to_csv(\"./objects/loss_df.csv\",index=False)\n",
    "            actions_df.to_csv(\"./objects/actions_df.csv\",index=False)\n",
    "            q_values_df.to_csv(q_value_file_path,index=False)\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "            clear_output()\n",
    "            game_state._game.resume()\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state,             \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t,             \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function\n",
    "def playGame(observe=False):\n",
    "    game = Game()\n",
    "    agent = Agent(game)\n",
    "    game_state = Game_sate(agent, game)\n",
    "    game.enter_level()\n",
    "    model = buildmodel()\n",
    "    try:\n",
    "        trainNetwork(model,game_state,observe=observe)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 28000 / STATE explore / EPSILON 0.07212889899995742 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.012976 / Loss  0.18924832344055176\n",
      "fps: 0.27484845768374455\n",
      "TIMESTEP 28001 / STATE explore / EPSILON 0.07212789999995742 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.046127 / Loss  0.050572384148836136\n",
      "fps: 3.841676512882514\n",
      "TIMESTEP 28002 / STATE explore / EPSILON 0.07212690099995742 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.06317 / Loss  0.015610002912580967\n",
      "fps: 5.050883298611529\n",
      "TIMESTEP 28003 / STATE explore / EPSILON 0.07212590199995741 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.079504 / Loss  0.050195351243019104\n",
      "fps: 5.490717249865819\n",
      "TIMESTEP 28004 / STATE explore / EPSILON 0.07212490299995741 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.0782 / Loss  0.01841265708208084\n",
      "fps: 5.683331052396948\n",
      "TIMESTEP 28005 / STATE explore / EPSILON 0.07212390399995741 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.10409 / Loss  0.0043601393699646\n",
      "fps: 5.840835538225874\n",
      "TIMESTEP 28006 / STATE explore / EPSILON 0.07212290499995741 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.099487 / Loss  0.04360600560903549\n",
      "fps: 6.388633504232886\n",
      "TIMESTEP 28007 / STATE explore / EPSILON 0.07212190599995741 / ACTION 0 / REWARD -0.2 / Q_MAX  -10.774808 / Loss  0.020184822380542755\n",
      "fps: 5.711097161951109\n",
      "TIMESTEP 28008 / STATE explore / EPSILON 0.07212090699995741 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.134905 / Loss  0.024837978184223175\n",
      "fps: 5.570479924351088\n",
      "TIMESTEP 28009 / STATE explore / EPSILON 0.0721199079999574 / ACTION 0 / REWARD -0.2 / Q_MAX  -10.851442 / Loss  0.002947999397292733\n",
      "fps: 5.097964000821643\n",
      "TIMESTEP 28010 / STATE explore / EPSILON 0.0721189089999574 / ACTION 0 / REWARD -0.2 / Q_MAX  -10.916246 / Loss  0.011297203600406647\n",
      "fps: 3.589007173215659\n",
      "TIMESTEP 28011 / STATE explore / EPSILON 0.0721179099999574 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.157015 / Loss  0.01618596538901329\n",
      "fps: 5.813923573377099\n",
      "TIMESTEP 28012 / STATE explore / EPSILON 0.0721169109999574 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.141434 / Loss  0.015375854447484016\n",
      "fps: 6.082103901602926\n",
      "TIMESTEP 28013 / STATE explore / EPSILON 0.0721159119999574 / ACTION 0 / REWARD -0.2 / Q_MAX  -11.107973 / Loss  0.014356425032019615\n",
      "fps: 5.1470294441758355\n",
      "TIMESTEP 28014 / STATE explore / EPSILON 0.0721149129999574 / ACTION 0 / REWARD -0.2 / Q_MAX  -11.19309 / Loss  0.012329697608947754\n",
      "fps: 6.0401495383091115\n",
      "TIMESTEP 28015 / STATE explore / EPSILON 0.0721139139999574 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.14816 / Loss  0.006781203672289848\n",
      "fps: 5.560974704304604\n",
      "TIMESTEP 28016 / STATE explore / EPSILON 0.0721129149999574 / ACTION 0 / REWARD -0.2 / Q_MAX  -11.311479 / Loss  0.019856227561831474\n",
      "fps: 4.66825306018872\n",
      "TIMESTEP 28017 / STATE explore / EPSILON 0.0721119159999574 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.151726 / Loss  0.0047302404418587685\n",
      "fps: 5.487190288612616\n",
      "TIMESTEP 28018 / STATE explore / EPSILON 0.07211091699995739 / ACTION 0 / REWARD -0.2 / Q_MAX  29.67865 / Loss  0.007921731099486351\n",
      "fps: 5.854801943642193\n",
      "TIMESTEP 28019 / STATE explore / EPSILON 0.07210991799995739 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.150438 / Loss  0.0016258488176390529\n",
      "fps: 4.717991635564374\n",
      "TIMESTEP 28020 / STATE explore / EPSILON 0.07210891899995739 / ACTION 0 / REWARD -0.2 / Q_MAX  29.181341 / Loss  0.01283898577094078\n",
      "fps: 5.552898312803839\n",
      "TIMESTEP 28021 / STATE explore / EPSILON 0.07210791999995739 / ACTION 0 / REWARD -0.2 / Q_MAX  -11.640872 / Loss  0.012645067647099495\n",
      "fps: 5.454579030393354\n",
      "TIMESTEP 28022 / STATE explore / EPSILON 0.07210692099995739 / ACTION 0 / REWARD -0.2 / Q_MAX  28.81854 / Loss  0.011095858179032803\n",
      "----------Random Action----------\n",
      "fps: 3.479534536066652\n",
      "TIMESTEP 28023 / STATE explore / EPSILON 0.07210592199995738 / ACTION 3 / REWARD -1 / Q_MAX  -11.83004 / Loss  0.003997521474957466\n",
      "fps: 3.398900178117337\n",
      "TIMESTEP 28024 / STATE explore / EPSILON 0.07210492299995738 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.165894 / Loss  0.011223044246435165\n",
      "fps: 4.31540283433717\n",
      "TIMESTEP 28025 / STATE explore / EPSILON 0.07210392399995738 / ACTION 0 / REWARD -0.2 / Q_MAX  -11.935328 / Loss  0.017713099718093872\n",
      "fps: 4.063362154569013\n",
      "TIMESTEP 28026 / STATE explore / EPSILON 0.07210292499995738 / ACTION 0 / REWARD -0.2 / Q_MAX  28.19321 / Loss  0.012216579169034958\n",
      "fps: 5.203489583850254\n",
      "TIMESTEP 28027 / STATE explore / EPSILON 0.07210192599995738 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.142797 / Loss  0.024896489456295967\n",
      "----------Random Action----------\n",
      "fps: 5.004192526945993\n",
      "TIMESTEP 28028 / STATE explore / EPSILON 0.07210092699995738 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.135172 / Loss  0.0166960209608078\n",
      "fps: 5.14191843159562\n",
      "TIMESTEP 28029 / STATE explore / EPSILON 0.07209992799995738 / ACTION 0 / REWARD -0.2 / Q_MAX  27.871254 / Loss  0.013233066536486149\n",
      "fps: 4.340595075841072\n",
      "TIMESTEP 28030 / STATE explore / EPSILON 0.07209892899995737 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.126225 / Loss  0.0046684336848556995\n",
      "fps: 5.293619341387196\n",
      "TIMESTEP 28031 / STATE explore / EPSILON 0.07209792999995737 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.109383 / Loss  0.016991879791021347\n",
      "fps: 4.436614116591777\n",
      "TIMESTEP 28032 / STATE explore / EPSILON 0.07209693099995737 / ACTION 0 / REWARD -0.2 / Q_MAX  -12.27189 / Loss  0.009630771353840828\n",
      "fps: 4.867317761521037\n",
      "TIMESTEP 28033 / STATE explore / EPSILON 0.07209593199995737 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.138403 / Loss  0.01611052080988884\n",
      "fps: 3.445265403384724\n",
      "TIMESTEP 28034 / STATE explore / EPSILON 0.07209493299995737 / ACTION 0 / REWARD -0.2 / Q_MAX  -12.3936615 / Loss  0.014831004664301872\n",
      "fps: 2.4717609696332525\n",
      "TIMESTEP 28035 / STATE explore / EPSILON 0.07209393399995737 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.151354 / Loss  0.004846083465963602\n",
      "fps: 3.182624161909011\n",
      "TIMESTEP 28036 / STATE explore / EPSILON 0.07209293499995736 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.17702 / Loss  0.020534545183181763\n",
      "fps: 4.66357637248089\n",
      "TIMESTEP 28037 / STATE explore / EPSILON 0.07209193599995736 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.20026 / Loss  0.009277981705963612\n",
      "fps: 4.67791634787613\n",
      "TIMESTEP 28038 / STATE explore / EPSILON 0.07209093699995736 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.193523 / Loss  0.009686864912509918\n",
      "fps: 4.740446048840913\n",
      "TIMESTEP 28039 / STATE explore / EPSILON 0.07208993799995736 / ACTION 0 / REWARD -0.2 / Q_MAX  27.003693 / Loss  0.014473083429038525\n",
      "fps: 4.774887324697265\n",
      "TIMESTEP 28040 / STATE explore / EPSILON 0.07208893899995736 / ACTION 0 / REWARD -0.2 / Q_MAX  26.769709 / Loss  0.017961684614419937\n",
      "fps: 5.361756851595946\n",
      "TIMESTEP 28041 / STATE explore / EPSILON 0.07208793999995736 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.215359 / Loss  0.0034718464594334364\n",
      "fps: 5.035390442575618\n",
      "TIMESTEP 28042 / STATE explore / EPSILON 0.07208694099995736 / ACTION 0 / REWARD -0.2 / Q_MAX  -12.865347 / Loss  2.300055742263794\n",
      "fps: 3.367282565500299\n",
      "TIMESTEP 28043 / STATE explore / EPSILON 0.07208594199995735 / ACTION 0 / REWARD -0.2 / Q_MAX  -13.021205 / Loss  0.018122658133506775\n",
      "fps: 4.385649905371352\n",
      "TIMESTEP 28044 / STATE explore / EPSILON 0.07208494299995735 / ACTION 0 / REWARD -0.2 / Q_MAX  26.072348 / Loss  0.011865489184856415\n",
      "fps: 5.082052108462567\n",
      "TIMESTEP 28045 / STATE explore / EPSILON 0.07208394399995735 / ACTION 0 / REWARD -0.2 / Q_MAX  25.89445 / Loss  0.02967880479991436\n",
      "fps: 4.326036126075113\n",
      "TIMESTEP 28046 / STATE explore / EPSILON 0.07208294499995735 / ACTION 0 / REWARD -0.2 / Q_MAX  -13.176058 / Loss  0.005542292259633541\n",
      "fps: 5.182900879571163\n",
      "TIMESTEP 28047 / STATE explore / EPSILON 0.07208194599995735 / ACTION 0 / REWARD -0.2 / Q_MAX  -13.317497 / Loss  0.022763635963201523\n",
      "fps: 3.675586811260817\n",
      "TIMESTEP 28048 / STATE explore / EPSILON 0.07208094699995735 / ACTION 1 / REWARD -1 / Q_MAX  -19.836811 / Loss  0.007909594103693962\n",
      "fps: 3.619670456085119\n",
      "TIMESTEP 28049 / STATE explore / EPSILON 0.07207994799995734 / ACTION 1 / REWARD -1 / Q_MAX  -19.720978 / Loss  0.025642206892371178\n",
      "fps: 3.4253365063446615\n",
      "TIMESTEP 28050 / STATE explore / EPSILON 0.07207894899995734 / ACTION 1 / REWARD -1 / Q_MAX  24.816202 / Loss  0.04522890970110893\n",
      "fps: 2.852088387932066\n",
      "TIMESTEP 28051 / STATE explore / EPSILON 0.07207794999995734 / ACTION 1 / REWARD -1 / Q_MAX  24.665506 / Loss  0.0044529009610414505\n",
      "fps: 3.574429935087126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 28052 / STATE explore / EPSILON 0.07207695099995734 / ACTION 0 / REWARD -0.2 / Q_MAX  24.366228 / Loss  0.0117575004696846\n",
      "fps: 3.998017340643754\n",
      "TIMESTEP 28053 / STATE explore / EPSILON 0.07207595199995734 / ACTION 0 / REWARD -0.2 / Q_MAX  24.012728 / Loss  0.018365751951932907\n",
      "fps: 4.319291333312051\n",
      "TIMESTEP 28054 / STATE explore / EPSILON 0.07207495299995734 / ACTION 0 / REWARD -0.2 / Q_MAX  -13.9468565 / Loss  0.009118248708546162\n",
      "fps: 4.9267721409701775\n",
      "TIMESTEP 28055 / STATE explore / EPSILON 0.07207395399995734 / ACTION 0 / REWARD -0.2 / Q_MAX  -14.043505 / Loss  0.01009825523942709\n",
      "fps: 4.933894835901659\n",
      "TIMESTEP 28056 / STATE explore / EPSILON 0.07207295499995733 / ACTION 0 / REWARD -0.2 / Q_MAX  23.716202 / Loss  0.0113978311419487\n",
      "fps: 6.151313695727097\n",
      "TIMESTEP 28057 / STATE explore / EPSILON 0.07207195599995733 / ACTION 0 / REWARD -0.2 / Q_MAX  23.388638 / Loss  0.02670690044760704\n",
      "fps: 6.234018867167354\n",
      "TIMESTEP 28058 / STATE explore / EPSILON 0.07207095699995733 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.363478 / Loss  0.01443666871637106\n",
      "----------Random Action----------\n",
      "fps: 4.621272619897577\n",
      "TIMESTEP 28059 / STATE explore / EPSILON 0.07206995799995733 / ACTION 4 / REWARD -1 / Q_MAX  23.03322 / Loss  0.047598205506801605\n",
      "fps: 5.361893938328627\n",
      "TIMESTEP 28060 / STATE explore / EPSILON 0.07206895899995733 / ACTION 0 / REWARD -0.2 / Q_MAX  -14.702794 / Loss  0.011746041476726532\n",
      "fps: 4.272220762262137\n",
      "TIMESTEP 28061 / STATE explore / EPSILON 0.07206795999995733 / ACTION 0 / REWARD -0.2 / Q_MAX  22.366709 / Loss  0.007220749277621508\n",
      "fps: 6.227363970020549\n",
      "TIMESTEP 28062 / STATE explore / EPSILON 0.07206696099995732 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.306452 / Loss  0.011714396066963673\n",
      "fps: 5.862674633958836\n",
      "TIMESTEP 28063 / STATE explore / EPSILON 0.07206596199995732 / ACTION 0 / REWARD -0.2 / Q_MAX  21.824564 / Loss  0.007090184837579727\n",
      "fps: 5.1965849240578\n",
      "TIMESTEP 28064 / STATE explore / EPSILON 0.07206496299995732 / ACTION 0 / REWARD -0.2 / Q_MAX  21.627087 / Loss  0.006814361549913883\n",
      "fps: 6.662691157244953\n",
      "TIMESTEP 28065 / STATE explore / EPSILON 0.07206396399995732 / ACTION 0 / REWARD -0.2 / Q_MAX  -15.307086 / Loss  0.010924253612756729\n",
      "fps: 7.111062886129516\n",
      "TIMESTEP 28066 / STATE explore / EPSILON 0.07206296499995732 / ACTION 0 / REWARD -0.2 / Q_MAX  -15.430529 / Loss  0.0034148960839957\n",
      "fps: 6.96301436494906\n",
      "TIMESTEP 28067 / STATE explore / EPSILON 0.07206196599995732 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.347322 / Loss  0.00936750415712595\n",
      "fps: 6.518157518493194\n",
      "TIMESTEP 28068 / STATE explore / EPSILON 0.07206096699995732 / ACTION 0 / REWARD -0.2 / Q_MAX  20.673035 / Loss  0.01582440920174122\n",
      "fps: 4.797492765392842\n",
      "TIMESTEP 28069 / STATE explore / EPSILON 0.07205996799995731 / ACTION 0 / REWARD -0.2 / Q_MAX  20.602816 / Loss  0.024776432663202286\n",
      "fps: 6.696219001898242\n",
      "TIMESTEP 28070 / STATE explore / EPSILON 0.07205896899995731 / ACTION 0 / REWARD -0.2 / Q_MAX  -15.876168 / Loss  0.026164796203374863\n",
      "fps: 6.8207828533329\n",
      "TIMESTEP 28071 / STATE explore / EPSILON 0.07205796999995731 / ACTION 0 / REWARD -0.2 / Q_MAX  20.312696 / Loss  0.0140412338078022\n",
      "----------Random Action----------\n",
      "fps: 6.784671054653568\n",
      "TIMESTEP 28072 / STATE explore / EPSILON 0.07205697099995731 / ACTION 0 / REWARD -0.2 / Q_MAX  -16.082396 / Loss  0.015005286782979965\n",
      "fps: 6.653517114035936\n",
      "TIMESTEP 28073 / STATE explore / EPSILON 0.07205597199995731 / ACTION 0 / REWARD -0.2 / Q_MAX  -16.196579 / Loss  0.007530795875936747\n",
      "fps: 6.425422853790097\n",
      "TIMESTEP 28074 / STATE explore / EPSILON 0.0720549729999573 / ACTION 0 / REWARD -0.2 / Q_MAX  -16.315708 / Loss  0.01104369293898344\n",
      "fps: 5.979468273621394\n",
      "TIMESTEP 28075 / STATE explore / EPSILON 0.0720539739999573 / ACTION 0 / REWARD -0.2 / Q_MAX  19.3002 / Loss  0.006231382954865694\n",
      "fps: 5.208283973704667\n",
      "TIMESTEP 28076 / STATE explore / EPSILON 0.0720529749999573 / ACTION 0 / REWARD -0.2 / Q_MAX  -16.605297 / Loss  0.015523535199463367\n",
      "fps: 5.646263155485793\n",
      "TIMESTEP 28077 / STATE explore / EPSILON 0.0720519759999573 / ACTION 0 / REWARD -0.2 / Q_MAX  -16.689154 / Loss  0.006677523255348206\n",
      "fps: 5.479083280209847\n",
      "TIMESTEP 28078 / STATE explore / EPSILON 0.0720509769999573 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.460554 / Loss  0.005522490944713354\n",
      "fps: 5.902474102907258\n",
      "TIMESTEP 28079 / STATE explore / EPSILON 0.0720499779999573 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.470034 / Loss  0.001762074651196599\n",
      "fps: 6.0667789098525216\n",
      "TIMESTEP 28080 / STATE explore / EPSILON 0.0720489789999573 / ACTION 0 / REWARD -0.2 / Q_MAX  18.581533 / Loss  0.012019643560051918\n",
      "fps: 6.703560748813697\n",
      "TIMESTEP 28081 / STATE explore / EPSILON 0.0720479799999573 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.472895 / Loss  0.015034109354019165\n",
      "fps: 6.400820417656072\n",
      "TIMESTEP 28082 / STATE explore / EPSILON 0.0720469809999573 / ACTION 0 / REWARD -0.2 / Q_MAX  -17.261606 / Loss  0.006455651018768549\n",
      "fps: 6.426377801951982\n",
      "TIMESTEP 28083 / STATE explore / EPSILON 0.07204598199995729 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.501429 / Loss  0.008410954847931862\n",
      "fps: 6.553825287744266\n",
      "TIMESTEP 28084 / STATE explore / EPSILON 0.07204498299995729 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.509472 / Loss  0.013201174326241016\n",
      "fps: 6.819607307539721\n",
      "TIMESTEP 28085 / STATE explore / EPSILON 0.07204398399995729 / ACTION 0 / REWARD -0.2 / Q_MAX  -17.542519 / Loss  0.003145103808492422\n",
      "----------Random Action----------\n",
      "fps: 4.7631963568845\n",
      "TIMESTEP 28086 / STATE explore / EPSILON 0.07204298499995729 / ACTION 1 / REWARD -1 / Q_MAX  -19.547016 / Loss  0.03546691685914993\n",
      "fps: 6.847563772907228\n",
      "TIMESTEP 28087 / STATE explore / EPSILON 0.07204198599995729 / ACTION 0 / REWARD -0.2 / Q_MAX  -17.716087 / Loss  0.04465918242931366\n",
      "fps: 7.068293233703574\n",
      "TIMESTEP 28088 / STATE explore / EPSILON 0.07204098699995728 / ACTION 0 / REWARD -0.2 / Q_MAX  17.004877 / Loss  0.003197022248059511\n",
      "fps: 7.1632361681342775\n",
      "TIMESTEP 28089 / STATE explore / EPSILON 0.07203998799995728 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.54618 / Loss  0.0036785691045224667\n",
      "fps: 7.061224833372896\n",
      "TIMESTEP 28090 / STATE explore / EPSILON 0.07203898899995728 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.542616 / Loss  0.018614571541547775\n",
      "fps: 6.656357519202691\n",
      "TIMESTEP 28091 / STATE explore / EPSILON 0.07203798999995728 / ACTION 0 / REWARD -0.2 / Q_MAX  -17.938591 / Loss  0.01431221328675747\n",
      "fps: 6.015408886211743\n",
      "TIMESTEP 28092 / STATE explore / EPSILON 0.07203699099995728 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.573578 / Loss  0.00545364897698164\n",
      "fps: 6.963210880017\n",
      "TIMESTEP 28093 / STATE explore / EPSILON 0.07203599199995728 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.582876 / Loss  0.018191419541835785\n",
      "fps: 6.887933851724733\n",
      "TIMESTEP 28094 / STATE explore / EPSILON 0.07203499299995728 / ACTION 0 / REWARD -0.2 / Q_MAX  15.750372 / Loss  0.0029410605784505606\n",
      "fps: 6.1368323715660225\n",
      "TIMESTEP 28095 / STATE explore / EPSILON 0.07203399399995727 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.600698 / Loss  0.014652428217232227\n",
      "fps: 6.2164084094765935\n",
      "TIMESTEP 28096 / STATE explore / EPSILON 0.07203299499995727 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.107819 / Loss  0.03127544745802879\n",
      "fps: 7.090698232694642\n",
      "TIMESTEP 28097 / STATE explore / EPSILON 0.07203199599995727 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.606838 / Loss  0.0026668203063309193\n",
      "fps: 6.896382538980482\n",
      "TIMESTEP 28098 / STATE explore / EPSILON 0.07203099699995727 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.235373 / Loss  0.009559649974107742\n",
      "fps: 6.5239872142851585\n",
      "TIMESTEP 28099 / STATE explore / EPSILON 0.07202999799995727 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.639284 / Loss  0.014690253883600235\n",
      "fps: 6.113201167166589\n",
      "TIMESTEP 28100 / STATE explore / EPSILON 0.07202899899995727 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.379757 / Loss  0.007202583365142345\n",
      "fps: 6.404778956466235\n",
      "TIMESTEP 28101 / STATE explore / EPSILON 0.07202799999995727 / ACTION 0 / REWARD -0.2 / Q_MAX  15.181328 / Loss  0.008645815774798393\n",
      "----------Random Action----------\n",
      "fps: 5.076448631612582\n",
      "TIMESTEP 28102 / STATE explore / EPSILON 0.07202700099995726 / ACTION 4 / REWARD -1 / Q_MAX  -19.666998 / Loss  0.00809453334659338\n",
      "fps: 5.249586346557727\n",
      "TIMESTEP 28103 / STATE explore / EPSILON 0.07202600199995726 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.603172 / Loss  0.011814366094768047\n",
      "fps: 6.972981240482254\n",
      "TIMESTEP 28104 / STATE explore / EPSILON 0.07202500299995726 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.68616 / Loss  0.011008640751242638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps: 6.544274408967222\n",
      "TIMESTEP 28105 / STATE explore / EPSILON 0.07202400399995726 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.684782 / Loss  0.007780664600431919\n",
      "fps: 6.431570989798249\n",
      "TIMESTEP 28106 / STATE explore / EPSILON 0.07202300499995726 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.76381 / Loss  0.010284430347383022\n",
      "fps: 7.372604165201862\n",
      "TIMESTEP 28107 / STATE explore / EPSILON 0.07202200599995726 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.715277 / Loss  0.011205269955098629\n",
      "----------Random Action----------\n",
      "fps: 5.22180349564072\n",
      "TIMESTEP 28108 / STATE explore / EPSILON 0.07202100699995725 / ACTION 4 / REWARD -1 / Q_MAX  14.922478 / Loss  0.008248036727309227\n",
      "fps: 7.772296015180266\n",
      "TIMESTEP 28109 / STATE explore / EPSILON 0.07202000799995725 / ACTION 0 / REWARD -0.2 / Q_MAX  14.729747 / Loss  0.004142895806580782\n",
      "fps: 7.26574520459195\n",
      "TIMESTEP 28110 / STATE explore / EPSILON 0.07201900899995725 / ACTION 0 / REWARD -0.2 / Q_MAX  14.583232 / Loss  0.008617683313786983\n",
      "fps: 4.685394224893793\n",
      "TIMESTEP 28111 / STATE explore / EPSILON 0.07201800999995725 / ACTION 0 / REWARD -0.2 / Q_MAX  14.571767 / Loss  0.0008447250002063811\n",
      "fps: 6.774829955047722\n",
      "TIMESTEP 28112 / STATE explore / EPSILON 0.07201701099995725 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.167223 / Loss  0.01684759557247162\n",
      "fps: 6.820960329411918\n",
      "TIMESTEP 28113 / STATE explore / EPSILON 0.07201601199995725 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.758127 / Loss  0.03237210959196091\n",
      "fps: 6.729340944606489\n",
      "TIMESTEP 28114 / STATE explore / EPSILON 0.07201501299995725 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.797659 / Loss  0.015089184045791626\n",
      "fps: 7.265770377415035\n",
      "TIMESTEP 28115 / STATE explore / EPSILON 0.07201401399995724 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.375526 / Loss  0.0031665314454585314\n",
      "fps: 7.596037452233913\n",
      "TIMESTEP 28116 / STATE explore / EPSILON 0.07201301499995724 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.464878 / Loss  0.0013928788248449564\n",
      "fps: 7.538885174240954\n",
      "TIMESTEP 28117 / STATE explore / EPSILON 0.07201201599995724 / ACTION 0 / REWARD -0.2 / Q_MAX  13.42419 / Loss  0.014009726233780384\n",
      "----------Random Action----------\n",
      "fps: 5.546854563656585\n",
      "TIMESTEP 28118 / STATE explore / EPSILON 0.07201101699995724 / ACTION 1 / REWARD -1 / Q_MAX  -19.586798 / Loss  0.022809628397226334\n",
      "fps: 7.548029268538481\n",
      "TIMESTEP 28119 / STATE explore / EPSILON 0.07201001799995724 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.55919 / Loss  0.00350317545235157\n",
      "fps: 7.672264050164263\n",
      "TIMESTEP 28120 / STATE explore / EPSILON 0.07200901899995724 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.866312 / Loss  0.001652721082791686\n",
      "fps: 7.8548401897460005\n",
      "TIMESTEP 28121 / STATE explore / EPSILON 0.07200801999995723 / ACTION 0 / REWARD -0.2 / Q_MAX  12.750606 / Loss  0.016654368489980698\n",
      "fps: 7.817974072451747\n",
      "TIMESTEP 28122 / STATE explore / EPSILON 0.07200702099995723 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.666273 / Loss  0.003388421842828393\n",
      "----------Random Action----------\n",
      "fps: 5.6102459153441275\n",
      "TIMESTEP 28123 / STATE explore / EPSILON 0.07200602199995723 / ACTION 4 / REWARD -1 / Q_MAX  -19.856749 / Loss  0.0023809196427464485\n",
      "fps: 7.83141825406013\n",
      "TIMESTEP 28124 / STATE explore / EPSILON 0.07200502299995723 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.749668 / Loss  0.023425258696079254\n",
      "fps: 7.334545759771691\n",
      "TIMESTEP 28125 / STATE explore / EPSILON 0.07200402399995723 / ACTION 0 / REWARD -0.2 / Q_MAX  12.217169 / Loss  0.017619218677282333\n",
      "fps: 7.482533101654458\n",
      "TIMESTEP 28126 / STATE explore / EPSILON 0.07200302499995723 / ACTION 0 / REWARD -0.2 / Q_MAX  12.042855 / Loss  0.016482045873999596\n",
      "fps: 7.285040869000351\n",
      "TIMESTEP 28127 / STATE explore / EPSILON 0.07200202599995723 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.883516 / Loss  0.0022157575003802776\n",
      "fps: 7.0609989730812615\n",
      "TIMESTEP 28128 / STATE explore / EPSILON 0.07200102699995722 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.820234 / Loss  0.001641560927964747\n",
      "fps: 6.786614155391521\n",
      "TIMESTEP 28129 / STATE explore / EPSILON 0.07200002799995722 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.873096 / Loss  0.0047653596848249435\n",
      "----------Random Action----------\n",
      "fps: 4.93160915891235\n",
      "TIMESTEP 28130 / STATE explore / EPSILON 0.07199902899995722 / ACTION 3 / REWARD -1 / Q_MAX  -19.876886 / Loss  0.0062240855768322945\n",
      "fps: 6.9832440928101445\n",
      "TIMESTEP 28131 / STATE explore / EPSILON 0.07199802999995722 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.888426 / Loss  0.010120312683284283\n",
      "fps: 7.290156395023447\n",
      "TIMESTEP 28132 / STATE explore / EPSILON 0.07199703099995722 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.885197 / Loss  0.01180298626422882\n",
      "fps: 7.422692964526205\n",
      "TIMESTEP 28133 / STATE explore / EPSILON 0.07199603199995722 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.878912 / Loss  0.006300092674791813\n",
      "----------Random Action----------\n",
      "fps: 5.283257293868389\n",
      "TIMESTEP 28134 / STATE explore / EPSILON 0.07199503299995721 / ACTION 4 / REWARD -1 / Q_MAX  11.239697 / Loss  0.0014783538645133376\n",
      "fps: 6.427461076376119\n",
      "TIMESTEP 28135 / STATE explore / EPSILON 0.07199403399995721 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.989609 / Loss  0.014862477779388428\n",
      "fps: 6.869624179848597\n",
      "TIMESTEP 28136 / STATE explore / EPSILON 0.07199303499995721 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.849741 / Loss  0.012005534023046494\n",
      "fps: 6.927764215422325\n",
      "TIMESTEP 28137 / STATE explore / EPSILON 0.07199203599995721 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.844858 / Loss  0.00949480663985014\n",
      "fps: 7.18962328350329\n",
      "TIMESTEP 28138 / STATE explore / EPSILON 0.07199103699995721 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.85425 / Loss  0.016896387562155724\n",
      "fps: 5.1417923621599355\n",
      "TIMESTEP 28139 / STATE explore / EPSILON 0.07199003799995721 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.852388 / Loss  0.0030776229687035084\n",
      "fps: 7.482933582627436\n",
      "TIMESTEP 28140 / STATE explore / EPSILON 0.0719890389999572 / ACTION 0 / REWARD -0.2 / Q_MAX  10.804176 / Loss  0.007876926101744175\n",
      "fps: 7.061058408599254\n",
      "TIMESTEP 28141 / STATE explore / EPSILON 0.0719880399999572 / ACTION 0 / REWARD -0.2 / Q_MAX  10.717295 / Loss  0.007181212306022644\n",
      "fps: 7.372630083916035\n",
      "TIMESTEP 28142 / STATE explore / EPSILON 0.0719870409999572 / ACTION 0 / REWARD -0.2 / Q_MAX  10.687536 / Loss  0.003472174983471632\n",
      "----------Random Action----------\n",
      "fps: 5.038541931949776\n",
      "TIMESTEP 28143 / STATE explore / EPSILON 0.0719860419999572 / ACTION 2 / REWARD -1 / Q_MAX  10.69987 / Loss  0.01579923927783966\n",
      "fps: 6.175777511433342\n",
      "TIMESTEP 28144 / STATE explore / EPSILON 0.0719850429999572 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.128092 / Loss  0.0017104654107242823\n",
      "fps: 5.968220179204381\n",
      "TIMESTEP 28145 / STATE explore / EPSILON 0.0719840439999572 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.159992 / Loss  0.006747990846633911\n",
      "----------Random Action----------\n",
      "fps: 3.324538292037222\n",
      "TIMESTEP 28146 / STATE explore / EPSILON 0.0719830449999572 / ACTION 1 / REWARD -1 / Q_MAX  -19.849876 / Loss  0.0026951285544782877\n",
      "fps: 3.5882610600087603\n",
      "TIMESTEP 28147 / STATE explore / EPSILON 0.0719820459999572 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.850414 / Loss  0.002611354924738407\n",
      "fps: 8.267212255243997\n",
      "TIMESTEP 28148 / STATE explore / EPSILON 0.0719810469999572 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.16578 / Loss  0.005514147225767374\n",
      "----------Random Action----------\n",
      "fps: 8.717954900220532\n",
      "TIMESTEP 28149 / STATE explore / EPSILON 0.07198004799995719 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.84166 / Loss  0.0019039910985156894\n",
      "fps: 7.770668870111031\n",
      "TIMESTEP 28150 / STATE explore / EPSILON 0.07197904899995719 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.839472 / Loss  0.0038369186222553253\n",
      "fps: 6.753829952369144\n",
      "TIMESTEP 28151 / STATE explore / EPSILON 0.07197804999995719 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.838316 / Loss  0.0116004329174757\n",
      "fps: 6.84051693206949\n",
      "TIMESTEP 28152 / STATE explore / EPSILON 0.07197705099995719 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.8382 / Loss  0.01323509868234396\n",
      "fps: 3.4041028071667645\n",
      "TIMESTEP 28153 / STATE explore / EPSILON 0.07197605199995719 / ACTION 0 / REWARD -0.2 / Q_MAX  10.291649 / Loss  0.0013760910369455814\n",
      "fps: 4.3255855205486515\n",
      "TIMESTEP 28154 / STATE explore / EPSILON 0.07197505299995718 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.827482 / Loss  0.0076768179424107075\n",
      "fps: 4.083160374603298\n",
      "TIMESTEP 28155 / STATE explore / EPSILON 0.07197405399995718 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.844383 / Loss  0.0023956100922077894\n",
      "fps: 9.181705344223323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 28156 / STATE explore / EPSILON 0.07197305499995718 / ACTION 0 / REWARD -0.2 / Q_MAX  10.112855 / Loss  0.003501851810142398\n",
      "fps: 9.025715183363245\n",
      "TIMESTEP 28157 / STATE explore / EPSILON 0.07197205599995718 / ACTION 0 / REWARD -0.2 / Q_MAX  10.043454 / Loss  0.0026914861518889666\n",
      "fps: 9.175579065270085\n",
      "TIMESTEP 28158 / STATE explore / EPSILON 0.07197105699995718 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.339779 / Loss  0.01009862869977951\n",
      "fps: 8.546426308207357\n",
      "TIMESTEP 28159 / STATE explore / EPSILON 0.07197005799995718 / ACTION 0 / REWARD -0.2 / Q_MAX  9.856117 / Loss  0.027552347630262375\n",
      "fps: 8.595008534958493\n",
      "TIMESTEP 28160 / STATE explore / EPSILON 0.07196905899995718 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.411255 / Loss  0.0005549761699512601\n",
      "fps: 6.151313695727097\n",
      "TIMESTEP 28161 / STATE explore / EPSILON 0.07196805999995717 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.871487 / Loss  0.009728130884468555\n",
      "fps: 6.499743531333342\n",
      "TIMESTEP 28162 / STATE explore / EPSILON 0.07196706099995717 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.45704 / Loss  0.00889181811362505\n",
      "fps: 3.728929828476326\n",
      "TIMESTEP 28163 / STATE explore / EPSILON 0.07196606199995717 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.432783 / Loss  0.012904602102935314\n",
      "fps: 8.310950478726777\n",
      "TIMESTEP 28164 / STATE explore / EPSILON 0.07196506299995717 / ACTION 0 / REWARD -0.2 / Q_MAX  9.061747 / Loss  0.015321895480155945\n",
      "fps: 7.4271618929567484\n",
      "TIMESTEP 28165 / STATE explore / EPSILON 0.07196406399995717 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.886856 / Loss  0.012290669605135918\n",
      "fps: 7.285559691576009\n",
      "TIMESTEP 28166 / STATE explore / EPSILON 0.07196306499995717 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.889599 / Loss  0.014059815555810928\n",
      "fps: 7.7728001363940455\n",
      "TIMESTEP 28167 / STATE explore / EPSILON 0.07196206599995716 / ACTION 0 / REWARD -0.2 / Q_MAX  8.648164 / Loss  0.007185144349932671\n",
      "fps: 7.8565910785265\n",
      "TIMESTEP 28168 / STATE explore / EPSILON 0.07196106699995716 / ACTION 0 / REWARD -0.2 / Q_MAX  8.422983 / Loss  0.0008692811825312674\n",
      "fps: 7.87986796447741\n",
      "TIMESTEP 28169 / STATE explore / EPSILON 0.07196006799995716 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.511572 / Loss  0.010669706389307976\n",
      "fps: 7.021753469606415\n",
      "TIMESTEP 28170 / STATE explore / EPSILON 0.07195906899995716 / ACTION 0 / REWARD -0.2 / Q_MAX  8.060452 / Loss  0.0029712722171097994\n",
      "fps: 6.7746876988729055\n",
      "TIMESTEP 28171 / STATE explore / EPSILON 0.07195806999995716 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.505192 / Loss  0.00979674607515335\n",
      "fps: 6.125280210528468\n",
      "TIMESTEP 28172 / STATE explore / EPSILON 0.07195707099995716 / ACTION 0 / REWARD -0.2 / Q_MAX  7.518242 / Loss  0.009829133749008179\n",
      "fps: 7.001393834078659\n",
      "TIMESTEP 28173 / STATE explore / EPSILON 0.07195607199995716 / ACTION 0 / REWARD -0.2 / Q_MAX  7.417962 / Loss  0.03198659047484398\n",
      "fps: 6.26389486260454\n",
      "TIMESTEP 28174 / STATE explore / EPSILON 0.07195507299995715 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.894344 / Loss  0.006895925384014845\n",
      "fps: 4.16956596512889\n",
      "TIMESTEP 28175 / STATE explore / EPSILON 0.07195407399995715 / ACTION 0 / REWARD -0.2 / Q_MAX  6.8457775 / Loss  0.014148535206913948\n",
      "----------Random Action----------\n",
      "fps: 6.5533747277437335\n",
      "TIMESTEP 28176 / STATE explore / EPSILON 0.07195307499995715 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.860199 / Loss  0.0283237025141716\n",
      "fps: 6.489234901322507\n",
      "TIMESTEP 28177 / STATE explore / EPSILON 0.07195207599995715 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.629242 / Loss  0.039096854627132416\n",
      "fps: 6.345977465393932\n",
      "TIMESTEP 28178 / STATE explore / EPSILON 0.07195107699995715 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.559149 / Loss  0.02057831361889839\n",
      "fps: 6.590102992356098\n",
      "TIMESTEP 28179 / STATE explore / EPSILON 0.07195007799995715 / ACTION 0 / REWARD -0.2 / Q_MAX  5.783221 / Loss  0.02291116490960121\n",
      "fps: 6.774786183280703\n",
      "TIMESTEP 28180 / STATE explore / EPSILON 0.07194907899995714 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.816809 / Loss  0.002132635796442628\n",
      "fps: 7.061105957734078\n",
      "TIMESTEP 28181 / STATE explore / EPSILON 0.07194807999995714 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.516449 / Loss  0.0007023446960374713\n",
      "fps: 7.270379942555334\n",
      "TIMESTEP 28182 / STATE explore / EPSILON 0.07194708099995714 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.481155 / Loss  0.003928599879145622\n",
      "fps: 6.915029123780601\n",
      "TIMESTEP 28183 / STATE explore / EPSILON 0.07194608199995714 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.484964 / Loss  0.0035072369500994682\n",
      "fps: 6.95027615154282\n",
      "TIMESTEP 28184 / STATE explore / EPSILON 0.07194508299995714 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.831726 / Loss  0.013827723450958729\n",
      "fps: 7.28421849682272\n",
      "TIMESTEP 28185 / STATE explore / EPSILON 0.07194408399995714 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.867775 / Loss  0.02263478934764862\n",
      "fps: 7.35437671177632\n",
      "TIMESTEP 28186 / STATE explore / EPSILON 0.07194308499995714 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.542486 / Loss  0.0075100501999258995\n",
      "----------Random Action----------\n",
      "fps: 5.375355639741428\n",
      "TIMESTEP 28187 / STATE explore / EPSILON 0.07194208599995713 / ACTION 4 / REWARD -1 / Q_MAX  -19.862865 / Loss  3.648789167404175\n",
      "fps: 7.566534313026882\n",
      "TIMESTEP 28188 / STATE explore / EPSILON 0.07194108699995713 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.861828 / Loss  0.003878816496580839\n",
      "fps: 7.316671696441188\n",
      "TIMESTEP 28189 / STATE explore / EPSILON 0.07194008799995713 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.847073 / Loss  0.029869768768548965\n",
      "fps: 7.381050342013153\n",
      "TIMESTEP 28190 / STATE explore / EPSILON 0.07193908899995713 / ACTION 0 / REWARD -0.2 / Q_MAX  3.5337954 / Loss  0.004508825019001961\n",
      "fps: 7.439809140333295\n",
      "TIMESTEP 28191 / STATE explore / EPSILON 0.07193808999995713 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.474894 / Loss  0.04405199736356735\n",
      "fps: 7.772684902922607\n",
      "TIMESTEP 28192 / STATE explore / EPSILON 0.07193709099995713 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.511362 / Loss  0.045075658708810806\n",
      "fps: 7.895086163894928\n",
      "TIMESTEP 28193 / STATE explore / EPSILON 0.07193609199995712 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.832174 / Loss  2.8470568656921387\n",
      "fps: 7.772684902922607\n",
      "TIMESTEP 28194 / STATE explore / EPSILON 0.07193509299995712 / ACTION 0 / REWARD -0.2 / Q_MAX  2.1546392 / Loss  0.00554150715470314\n",
      "fps: 7.265770377415035\n",
      "TIMESTEP 28195 / STATE explore / EPSILON 0.07193409399995712 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.749409 / Loss  0.0039578648284077644\n",
      "fps: 7.213549620946327\n",
      "TIMESTEP 28196 / STATE explore / EPSILON 0.07193309499995712 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.806223 / Loss  0.009590726345777512\n",
      "fps: 7.538966477936551\n",
      "TIMESTEP 28197 / STATE explore / EPSILON 0.07193209599995712 / ACTION 0 / REWARD -0.2 / Q_MAX  1.1611699 / Loss  0.21107646822929382\n",
      "----------Random Action----------\n",
      "fps: 7.595968669353013\n",
      "TIMESTEP 28198 / STATE explore / EPSILON 0.07193109699995712 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.81852 / Loss  0.011617891490459442\n",
      "fps: 7.8334805661959575\n",
      "TIMESTEP 28199 / STATE explore / EPSILON 0.07193009799995712 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.808498 / Loss  0.007750449702143669\n",
      "fps: 7.566520663005751\n",
      "TIMESTEP 28200 / STATE explore / EPSILON 0.07192909899995711 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.738937 / Loss  0.0061615691520273685\n",
      "fps: 7.65252675635931\n",
      "TIMESTEP 28201 / STATE explore / EPSILON 0.07192809999995711 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.679436 / Loss  0.02739907242357731\n",
      "fps: 7.536555213753846\n",
      "TIMESTEP 28202 / STATE explore / EPSILON 0.07192710099995711 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.630602 / Loss  0.0220660250633955\n",
      "fps: 7.579948747790697\n",
      "TIMESTEP 28203 / STATE explore / EPSILON 0.07192610199995711 / ACTION 0 / REWARD -0.2 / Q_MAX  -0.1790196 / Loss  0.00025191204622387886\n",
      "fps: 7.64986092999079\n",
      "TIMESTEP 28204 / STATE explore / EPSILON 0.07192510299995711 / ACTION 0 / REWARD -0.2 / Q_MAX  -0.28841183 / Loss  0.0029488117434084415\n",
      "----------Random Action----------\n",
      "fps: 5.583560972752493\n",
      "TIMESTEP 28205 / STATE explore / EPSILON 0.0719241039999571 / ACTION 3 / REWARD -1 / Q_MAX  -20.488972 / Loss  0.0033018356189131737\n",
      "fps: 7.713123496656785\n",
      "TIMESTEP 28206 / STATE explore / EPSILON 0.0719231049999571 / ACTION 0 / REWARD -0.2 / Q_MAX  -0.5490293 / Loss  0.014996826648712158\n",
      "fps: 7.769229486849393\n",
      "TIMESTEP 28207 / STATE explore / EPSILON 0.0719221059999571 / ACTION 0 / REWARD -0.2 / Q_MAX  -0.61043 / Loss  0.014820780605077744\n",
      "fps: 7.6540070220515775\n",
      "TIMESTEP 28208 / STATE explore / EPSILON 0.0719211069999571 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.406612 / Loss  0.37648648023605347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps: 5.615067746759253\n",
      "TIMESTEP 28209 / STATE explore / EPSILON 0.0719201079999571 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.870539 / Loss  0.0021577223669737577\n",
      "fps: 7.6540070220515775\n",
      "TIMESTEP 28210 / STATE explore / EPSILON 0.0719191089999571 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.29999 / Loss  0.0038840847555547953\n",
      "fps: 7.323135695167325\n",
      "TIMESTEP 28211 / STATE explore / EPSILON 0.0719181099999571 / ACTION 0 / REWARD -0.2 / Q_MAX  -0.7719152 / Loss  0.021630723029375076\n",
      "fps: 7.612609080745162\n",
      "TIMESTEP 28212 / STATE explore / EPSILON 0.0719171109999571 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.20197 / Loss  0.0056791459210217\n",
      "fps: 7.682452345322634\n",
      "TIMESTEP 28213 / STATE explore / EPSILON 0.0719161119999571 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.898952 / Loss  0.006368829868733883\n",
      "fps: 7.559865323749306\n",
      "TIMESTEP 28214 / STATE explore / EPSILON 0.07191511299995709 / ACTION 0 / REWARD -0.2 / Q_MAX  -0.84890264 / Loss  0.009480666369199753\n",
      "fps: 7.478824016090609\n",
      "TIMESTEP 28215 / STATE explore / EPSILON 0.07191411399995709 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.135738 / Loss  0.005405427888035774\n",
      "fps: 7.3231229092026515\n",
      "TIMESTEP 28216 / STATE explore / EPSILON 0.07191311499995709 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.162315 / Loss  0.00806608609855175\n",
      "fps: 7.208293232875959\n",
      "TIMESTEP 28217 / STATE explore / EPSILON 0.07191211599995709 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.915922 / Loss  0.23835410177707672\n",
      "fps: 7.34412635044037\n",
      "TIMESTEP 28218 / STATE explore / EPSILON 0.07191111699995709 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.920364 / Loss  0.010219976305961609\n",
      "fps: 7.3763772548288555\n",
      "TIMESTEP 28219 / STATE explore / EPSILON 0.07191011799995708 / ACTION 0 / REWARD -0.2 / Q_MAX  -0.8408375 / Loss  0.024434277787804604\n",
      "fps: 7.233915879484867\n",
      "TIMESTEP 28220 / STATE explore / EPSILON 0.07190911899995708 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.9308 / Loss  0.010460750199854374\n",
      "fps: 7.234614621693646\n",
      "TIMESTEP 28221 / STATE explore / EPSILON 0.07190811999995708 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.923725 / Loss  0.18785937130451202\n",
      "----------Random Action----------\n",
      "fps: 5.420180892210315\n",
      "TIMESTEP 28222 / STATE explore / EPSILON 0.07190712099995708 / ACTION 4 / REWARD -1 / Q_MAX  -0.8168849 / Loss  0.2865549921989441\n",
      "fps: 7.611227652485718\n",
      "TIMESTEP 28223 / STATE explore / EPSILON 0.07190612199995708 / ACTION 0 / REWARD -0.2 / Q_MAX  -0.6026945 / Loss  0.11464089900255203\n",
      "fps: 7.538898724735106\n",
      "TIMESTEP 28224 / STATE explore / EPSILON 0.07190512299995708 / ACTION 0 / REWARD -0.2 / Q_MAX  -0.40491375 / Loss  0.009146057069301605\n",
      "fps: 7.393697303452786\n",
      "TIMESTEP 28225 / STATE explore / EPSILON 0.07190412399995708 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.532272 / Loss  0.1092599481344223\n",
      "fps: 7.622016107928939\n",
      "TIMESTEP 28226 / STATE explore / EPSILON 0.07190312499995707 / ACTION 0 / REWARD -0.2 / Q_MAX  0.17503247 / Loss  0.06345202028751373\n",
      "fps: 7.596023695558079\n",
      "TIMESTEP 28227 / STATE explore / EPSILON 0.07190212599995707 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.963413 / Loss  0.03922297805547714\n",
      "fps: 7.48858493143103\n",
      "TIMESTEP 28228 / STATE explore / EPSILON 0.07190112699995707 / ACTION 0 / REWARD -0.2 / Q_MAX  0.8766136 / Loss  0.04732286185026169\n",
      "fps: 7.372591205913114\n",
      "TIMESTEP 28229 / STATE explore / EPSILON 0.07190012799995707 / ACTION 0 / REWARD -0.2 / Q_MAX  0.9583085 / Loss  0.00415391381829977\n",
      "fps: 7.47605127693251\n",
      "TIMESTEP 28230 / STATE explore / EPSILON 0.07189912899995707 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.928493 / Loss  0.027967143803834915\n",
      "fps: 7.608438696101729\n",
      "TIMESTEP 28231 / STATE explore / EPSILON 0.07189812999995707 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.86518 / Loss  0.011429990641772747\n",
      "fps: 7.714414462467146\n",
      "TIMESTEP 28232 / STATE explore / EPSILON 0.07189713099995707 / ACTION 0 / REWARD -0.2 / Q_MAX  -19.91527 / Loss  0.18226411938667297\n",
      "fps: 7.507901162083014\n",
      "TIMESTEP 28233 / STATE explore / EPSILON 0.07189613199995706 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.794758 / Loss  0.01640956848859787\n",
      "fps: 7.3130996821455785\n",
      "TIMESTEP 28234 / STATE explore / EPSILON 0.07189513299995706 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.716852 / Loss  0.005471376236528158\n",
      "fps: 7.462377682097997\n",
      "TIMESTEP 28235 / STATE explore / EPSILON 0.07189413399995706 / ACTION 0 / REWARD -0.2 / Q_MAX  2.6822793 / Loss  0.011783527210354805\n",
      "fps: 7.348114926419061\n",
      "TIMESTEP 28236 / STATE explore / EPSILON 0.07189313499995706 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.558285 / Loss  0.019170068204402924\n",
      "fps: 7.14902436696347\n",
      "TIMESTEP 28237 / STATE explore / EPSILON 0.07189213599995706 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.512377 / Loss  0.039354801177978516\n",
      "----------Random Action----------\n",
      "fps: 5.490329134072654\n",
      "TIMESTEP 28238 / STATE explore / EPSILON 0.07189113699995706 / ACTION 3 / REWARD -1 / Q_MAX  -20.450815 / Loss  0.015869852155447006\n",
      "fps: 7.335533484090881\n",
      "TIMESTEP 28239 / STATE explore / EPSILON 0.07189013799995705 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.35377 / Loss  0.018033718690276146\n",
      "fps: 6.427520174575591\n",
      "TIMESTEP 28240 / STATE explore / EPSILON 0.07188913899995705 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.67596 / Loss  0.032171014696359634\n",
      "fps: 7.458502859419012\n",
      "TIMESTEP 28241 / STATE explore / EPSILON 0.07188813999995705 / ACTION 0 / REWARD -0.2 / Q_MAX  4.4664736 / Loss  0.02507239207625389\n",
      "fps: 7.581359954739272\n",
      "TIMESTEP 28242 / STATE explore / EPSILON 0.07188714099995705 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.882797 / Loss  0.022869378328323364\n",
      "fps: 5.1289594005153045\n",
      "TIMESTEP 28243 / STATE explore / EPSILON 0.07188614199995705 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.97767 / Loss  0.012164904735982418\n",
      "fps: 6.842224263176526\n",
      "TIMESTEP 28244 / STATE explore / EPSILON 0.07188514299995705 / ACTION 0 / REWARD -0.2 / Q_MAX  5.1877823 / Loss  0.012349173426628113\n",
      "fps: 7.605637991343184\n",
      "TIMESTEP 28245 / STATE explore / EPSILON 0.07188414399995705 / ACTION 0 / REWARD -0.2 / Q_MAX  -21.14266 / Loss  0.006909244228154421\n",
      "fps: 7.315867689023157\n",
      "TIMESTEP 28246 / STATE explore / EPSILON 0.07188314499995704 / ACTION 0 / REWARD -0.2 / Q_MAX  -17.778538 / Loss  0.011401787400245667\n",
      "fps: 6.865519877300413\n",
      "TIMESTEP 28247 / STATE explore / EPSILON 0.07188214599995704 / ACTION 0 / REWARD -0.2 / Q_MAX  -21.254147 / Loss  0.0022486278321594\n",
      "fps: 7.088445353298068\n",
      "TIMESTEP 28248 / STATE explore / EPSILON 0.07188114699995704 / ACTION 0 / REWARD -0.2 / Q_MAX  6.1675973 / Loss  0.02304532378911972\n",
      "----------Random Action----------\n",
      "fps: 4.402257856631562\n",
      "TIMESTEP 28249 / STATE explore / EPSILON 0.07188014799995704 / ACTION 3 / REWARD -1 / Q_MAX  -17.67046 / Loss  0.009589148685336113\n",
      "fps: 8.320001269531444\n",
      "TIMESTEP 28250 / STATE explore / EPSILON 0.07187914899995704 / ACTION 0 / REWARD -0.2 / Q_MAX  -21.391346 / Loss  0.02247721701860428\n",
      "fps: 8.007881291859817\n",
      "TIMESTEP 28251 / STATE explore / EPSILON 0.07187814999995704 / ACTION 0 / REWARD -0.2 / Q_MAX  -21.41291 / Loss  0.011921578086912632\n",
      "fps: 7.318765497156637\n",
      "TIMESTEP 28252 / STATE explore / EPSILON 0.07187715099995703 / ACTION 0 / REWARD -0.2 / Q_MAX  -17.55872 / Loss  0.019794726744294167\n",
      "----------Random Action----------\n",
      "fps: 5.662338994811924\n",
      "TIMESTEP 28253 / STATE explore / EPSILON 0.07187615199995703 / ACTION 3 / REWARD -1 / Q_MAX  -17.51058 / Loss  0.023868249729275703\n",
      "fps: 7.957789207864625\n",
      "TIMESTEP 28254 / STATE explore / EPSILON 0.07187515299995703 / ACTION 0 / REWARD -0.2 / Q_MAX  -21.410522 / Loss  0.003737527644261718\n",
      "fps: 7.910318502089651\n",
      "TIMESTEP 28255 / STATE explore / EPSILON 0.07187415399995703 / ACTION 0 / REWARD -0.2 / Q_MAX  -17.557072 / Loss  0.006525516975671053\n",
      "fps: 7.162025234319905\n",
      "TIMESTEP 28256 / STATE explore / EPSILON 0.07187315499995703 / ACTION 0 / REWARD -0.2 / Q_MAX  -21.368002 / Loss  0.007645761594176292\n",
      "----------Random Action----------\n",
      "fps: 7.71288237511608\n",
      "TIMESTEP 28257 / STATE explore / EPSILON 0.07187215599995703 / ACTION 0 / REWARD -0.2 / Q_MAX  -21.361088 / Loss  0.004062780179083347\n",
      "fps: 7.292222367105664\n",
      "TIMESTEP 28258 / STATE explore / EPSILON 0.07187115699995703 / ACTION 0 / REWARD -0.2 / Q_MAX  -17.606497 / Loss  0.13932383060455322\n",
      "fps: 7.3901929345432125\n",
      "TIMESTEP 28259 / STATE explore / EPSILON 0.07187015799995702 / ACTION 0 / REWARD -0.2 / Q_MAX  -21.329025 / Loss  0.02175365388393402\n",
      "fps: 7.2089003012943795\n",
      "TIMESTEP 28260 / STATE explore / EPSILON 0.07186915899995702 / ACTION 0 / REWARD -0.2 / Q_MAX  -17.727383 / Loss  0.010839385911822319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps: 6.989842631616663\n",
      "TIMESTEP 28261 / STATE explore / EPSILON 0.07186815999995702 / ACTION 0 / REWARD -0.2 / Q_MAX  -17.843033 / Loss  3.5151267051696777\n",
      "fps: 6.580838501861612\n",
      "TIMESTEP 28262 / STATE explore / EPSILON 0.07186716099995702 / ACTION 0 / REWARD -0.2 / Q_MAX  5.9156284 / Loss  0.03010724112391472\n",
      "fps: 6.191457765627352\n",
      "TIMESTEP 28263 / STATE explore / EPSILON 0.07186616199995702 / ACTION 0 / REWARD -0.2 / Q_MAX  -21.213743 / Loss  0.004550321027636528\n",
      "fps: 6.468518764130545\n",
      "TIMESTEP 28264 / STATE explore / EPSILON 0.07186516299995702 / ACTION 0 / REWARD -0.2 / Q_MAX  5.1406093 / Loss  0.0402977392077446\n",
      "fps: 6.4063930985816535\n",
      "TIMESTEP 28265 / STATE explore / EPSILON 0.07186416399995701 / ACTION 0 / REWARD -0.2 / Q_MAX  4.9010835 / Loss  0.004968602675944567\n",
      "fps: 4.67016068241312\n",
      "TIMESTEP 28266 / STATE explore / EPSILON 0.07186316499995701 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.330647 / Loss  0.012765862978994846\n",
      "fps: 2.764495743486044\n",
      "TIMESTEP 28267 / STATE explore / EPSILON 0.07186216599995701 / ACTION 0 / REWARD -0.2 / Q_MAX  -21.456709 / Loss  0.07447903603315353\n",
      "fps: 5.073102290471508\n",
      "TIMESTEP 28268 / STATE explore / EPSILON 0.07186116699995701 / ACTION 0 / REWARD -0.2 / Q_MAX  -21.00748 / Loss  0.003040499519556761\n",
      "fps: 5.096873682279015\n",
      "TIMESTEP 28269 / STATE explore / EPSILON 0.07186016799995701 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.99031 / Loss  0.19072884321212769\n",
      "fps: 4.963414922773252\n",
      "TIMESTEP 28270 / STATE explore / EPSILON 0.07185916899995701 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.5842 / Loss  0.009145187214016914\n",
      "fps: 5.695524706655568\n",
      "TIMESTEP 28271 / STATE explore / EPSILON 0.071858169999957 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.898966 / Loss  0.0020729531534016132\n",
      "fps: 5.564589803767567\n",
      "TIMESTEP 28272 / STATE explore / EPSILON 0.071857170999957 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.513489 / Loss  0.06541814655065536\n",
      "----------Random Action----------\n",
      "fps: 3.950554770650843\n",
      "TIMESTEP 28273 / STATE explore / EPSILON 0.071856171999957 / ACTION 4 / REWARD -1 / Q_MAX  -18.490028 / Loss  0.0785612091422081\n",
      "fps: 5.519547308856429\n",
      "TIMESTEP 28274 / STATE explore / EPSILON 0.071855172999957 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.540289 / Loss  0.015525596216320992\n",
      "fps: 5.504877776684057\n",
      "TIMESTEP 28275 / STATE explore / EPSILON 0.071854173999957 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.767033 / Loss  0.002127560321241617\n",
      "fps: 5.338465219759799\n",
      "TIMESTEP 28276 / STATE explore / EPSILON 0.071853174999957 / ACTION 0 / REWARD -0.2 / Q_MAX  4.319457 / Loss  0.04721107706427574\n",
      "fps: 5.419809687239948\n",
      "TIMESTEP 28277 / STATE explore / EPSILON 0.071852175999957 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.546488 / Loss  0.015498601831495762\n",
      "fps: 3.8548816690409446\n",
      "TIMESTEP 28278 / STATE explore / EPSILON 0.071851176999957 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.709469 / Loss  0.03205020725727081\n",
      "fps: 5.468332514142451\n",
      "TIMESTEP 28279 / STATE explore / EPSILON 0.071850177999957 / ACTION 0 / REWARD -0.2 / Q_MAX  4.463841 / Loss  0.0022353692911565304\n",
      "fps: 5.206835680005164\n",
      "TIMESTEP 28280 / STATE explore / EPSILON 0.07184917899995699 / ACTION 0 / REWARD -0.2 / Q_MAX  4.405296 / Loss  0.03734883666038513\n",
      "fps: 5.4002848012442675\n",
      "TIMESTEP 28281 / STATE explore / EPSILON 0.07184817999995699 / ACTION 0 / REWARD -0.2 / Q_MAX  4.4668493 / Loss  0.020767567679286003\n",
      "fps: 5.539638430734445\n",
      "TIMESTEP 28282 / STATE explore / EPSILON 0.07184718099995699 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.621853 / Loss  0.0016528952401131392\n",
      "fps: 5.420993518285156\n",
      "TIMESTEP 28283 / STATE explore / EPSILON 0.07184618199995699 / ACTION 0 / REWARD -0.2 / Q_MAX  -18.613348 / Loss  0.016530536115169525\n",
      "fps: 5.352178168421886\n",
      "TIMESTEP 28284 / STATE explore / EPSILON 0.07184518299995699 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.619427 / Loss  0.01430077850818634\n",
      "----------Random Action----------\n",
      "fps: 2.0690899673182463\n",
      "TIMESTEP 28285 / STATE explore / EPSILON 0.07184418399995698 / ACTION 3 / REWARD -0.1 / Q_MAX  4.401305 / Loss  0.0013754558749496937\n",
      "fps: 5.898075038213775\n",
      "TIMESTEP 28286 / STATE explore / EPSILON 0.07184318499995698 / ACTION 0 / REWARD -0.2 / Q_MAX  -20.589142 / Loss  0.0073722777888178825\n",
      "fps: 4.7570701565953915\n",
      "TIMESTEP 28287 / STATE explore / EPSILON 0.07184218599995698 / ACTION 0 / REWARD -0.2 / Q_MAX  4.4024525 / Loss  0.021673940122127533\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: chrome not reachable\n  (Session info: chrome=71.0.3578.98)\n  (Driver info: chromedriver=2.45.615291 (ec3682e3c9061c10f26ea9e5cdcf3c53f3f74387),platform=Windows NT 10.0.17763 x86_64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-4829a0a02061>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplayGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-37f9ba776551>\u001b[0m in \u001b[0;36mplayGame\u001b[1;34m(observe)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuildmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mtrainNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgame_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-75e94dc02335>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[1;34m(model, game_state, observe)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m#run the selected action and observed next state and reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mx_t1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fps: {0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlast_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# helpful for measuring frame rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mlast_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-bf6744fba94c>\u001b[0m in \u001b[0;36mget_state\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[0maction_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_block_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#display the image on screen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-bf6744fba94c>\u001b[0m in \u001b[0;36mget_block_map\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# initiliaze the display coroutine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_block_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mscreen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrab_screen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_game\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mhsv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2HSV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mblock_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-a9be2f3c64aa>\u001b[0m in \u001b[0;36mgrab_screen\u001b[1;34m(_driver)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgrab_screen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_driver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mimage_b64\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_driver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetbase64Script\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mscreen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase64\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb64decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_b64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#processing image as required\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute_script\u001b[1;34m(self, script, *args)\u001b[0m\n\u001b[0;32m    634\u001b[0m         return self.execute(command, {\n\u001b[0;32m    635\u001b[0m             \u001b[1;34m'script'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m             'args': converted_args})['value']\n\u001b[0m\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexecute_async_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: chrome not reachable\n  (Session info: chrome=71.0.3578.98)\n  (Driver info: chromedriver=2.45.615291 (ec3682e3c9061c10f26ea9e5cdcf3c53f3f74387),platform=Windows NT 10.0.17763 x86_64)\n"
     ]
    }
   ],
   "source": [
    "playGame(observe=False);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
